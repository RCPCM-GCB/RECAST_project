{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "df_sns = pd.read_csv(\"../OUTPUT/df_sns\", sep = \"\\t\")\n",
    "df_sg = pd.read_csv(\"..//OUTPUT/df_sg\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  82 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=40)]: Done 285 tasks      | elapsed:   10.4s\n",
      "[Parallel(n_jobs=40)]: Done 568 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=40)]: Done 933 tasks      | elapsed:   25.9s\n",
      "[Parallel(n_jobs=40)]: Done 1378 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=40)]: Done 1905 tasks      | elapsed:   50.6s\n",
      "[Parallel(n_jobs=40)]: Done 2512 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=40)]: Done 3201 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=40)]: Done 3970 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=40)]: Done 4821 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=40)]: Done 5752 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=40)]: Done 6765 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=40)]: Done 7858 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=40)]: Done 9033 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=40)]: Done 10000 out of 10000 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_job...\n",
       "                                        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10, 11],\n",
       "                                        'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                         9, 10, 11, 12, 13, 14,\n",
       "                                                         15, 16, 17, 18, 19, 20,\n",
       "                                                         21, 22, 23, 24, 25, 26,\n",
       "                                                         27, 28, 29, 30, ...]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init dataset\n",
    "X = df_sns[['family', 'order', 'class', 'phyla', 'Donor', \n",
    "            'Subject', 'Dataset', 'Time', 'baseline_abundance', \n",
    "            'donor_abundance', 'mean_relab_hmp2012']]\n",
    "y = df_sns[['Status']]\n",
    "\n",
    "# Re-format string features\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for i in range(7):\n",
    "    X.iloc[:,i] = le.fit_transform(X.iloc[:,i])\n",
    "\n",
    "# Split train and test data frame\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# make griid_search \n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 500, num = 500)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 20, num = 20)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10, 11]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "criterion = ['entropy']\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion' : criterion}\n",
    "\n",
    "# Run greed search Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_random_sns = RandomizedSearchCV(estimator = rf,\n",
    "                                   param_distributions = random_grid, \n",
    "                                   n_iter = 1000, cv = 10, verbose=2, random_state=42, \n",
    "                                   n_jobs = 40)\n",
    "rf_random_sns.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Feature Importance DataFrame\n",
    "group = [\"Taxonomy\", \"Taxonomy\", \"Taxonomy\", \"Taxonomy\", \n",
    "         \"Metadata\", \"Metadata\", \"Metadata\",\"Metadata\", \n",
    "         \"Abundance\", \"Abundance\", \"Abundance\"]\n",
    "data_imp = {'Feature':  X.columns,\n",
    "        'Importance': rf_random_sns.best_estimator_.feature_importances_,\n",
    "         'Group' : group\n",
    "        }\n",
    "\n",
    "feature_importance_sns = pd.DataFrame(data_imp, columns = ['Feature','Importance', 'Group'])\n",
    "\n",
    "# Calculate metrics\n",
    "f1_score_sns = metrics.f1_score(rf_random_sns.best_estimator_.predict(X_test), y_test)\n",
    "roc_auc_score_sns = metrics.roc_auc_score(rf_random_sns.best_estimator_.predict(X_test), y_test)\n",
    "accuracy_score_sns = metrics.accuracy_score(rf_random_sns.best_estimator_.predict(X_test), y_test)\n",
    "precision_score_sns = metrics.precision_score(rf_random_sns.best_estimator_.predict(X_test), y_test)\n",
    "recall_score_sns = metrics.recall_score(rf_random_sns.best_estimator_.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1000 candidates, totalling 10000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done  82 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=40)]: Done 285 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=40)]: Done 568 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=40)]: Done 933 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=40)]: Done 1378 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=40)]: Done 1905 tasks      | elapsed:   42.2s\n",
      "[Parallel(n_jobs=40)]: Done 2512 tasks      | elapsed:   52.4s\n",
      "[Parallel(n_jobs=40)]: Done 3201 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=40)]: Done 3970 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=40)]: Done 4821 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=40)]: Done 5752 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=40)]: Done 6765 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=40)]: Done 7858 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=40)]: Done 9033 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=40)]: Done 10000 out of 10000 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_job...\n",
       "                                        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n",
       "                                                      10, 11, 12, 13, 14, 15,\n",
       "                                                      16, 17, 18, 19, 20,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                         9, 10, 11, 12, 13, 14,\n",
       "                                                         15, 16, 17, 18, 19, 20,\n",
       "                                                         21, 22, 23, 24, 25, 26,\n",
       "                                                         27, 28, 29, 30, ...]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init dataset\n",
    "X = df_sg[['family', 'order', 'class', 'phyla', 'Donor', \n",
    "            'Subject', 'Dataset', 'Time', 'baseline_abundance', \n",
    "            'donor_abundance', 'mean_relab_hmp2012']]\n",
    "y = df_sg[['Status']]\n",
    "\n",
    "# Re-format string features\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for i in range(7):\n",
    "    X.iloc[:,i] = le.fit_transform(X.iloc[:,i])\n",
    "\n",
    "# Split train and test data frame\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# make griid_search \n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 500, num = 500)]\n",
    "max_features = ['auto', 'sqrt']\n",
    "max_depth = [int(x) for x in np.linspace(start = 1, stop = 20, num = 20)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "bootstrap = [True, False]\n",
    "criterion = ['entropy']\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'criterion': criterion}\n",
    "\n",
    "# Run greed search Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf_random_sg = RandomizedSearchCV(estimator = rf, \n",
    "                                  param_distributions = random_grid, n_iter = 1000, \n",
    "                                  cv = 10, verbose=2, random_state=42, \n",
    "                                  n_jobs = 40)\n",
    "rf_random_sg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Feature Importance DataFrame\n",
    "group = [\"Taxonomy\", \"Taxonomyrecall_score\", \"Taxonomy\", \"Taxonomy\", \n",
    "         \"Metadata\", \"Metadata\", \"Metadata\",\"Metadata\", \n",
    "         \"Abundance\", \"Abundance\", \"Abundance\"]\n",
    "data_imp = {'Feature':  X.columns,\n",
    "        'Importance': rf_random_sg.best_estimator_.feature_importances_,\n",
    "         'Group' : group\n",
    "        }\n",
    "\n",
    "feature_importance_sg = pd.DataFrame(data_imp, columns = ['Feature','Importance', 'Group'])\n",
    "\n",
    "# Calculate metrics\n",
    "f1_score_sg = metrics.f1_score(rf_random_sg.best_estimator_.predict(X_test), y_test)\n",
    "roc_auc_score_sg = metrics.roc_auc_score(rf_random_sg.best_estimator_.predict(X_test), y_test)\n",
    "accuracy_score_sg = metrics.accuracy_score(rf_random_sg.best_estimator_.predict(X_test), y_test)\n",
    "precision_score_sg = metrics.precision_score(rf_random_sg.best_estimator_.predict(X_test), y_test)\n",
    "recall_score_sg = metrics.recall_score(rf_random_sg.best_estimator_.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=15, max_features='sqrt',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=487,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=13, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=2, min_samples_split=10,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=188,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "print(rf_random_sns.best_estimator_)\n",
    "print(rf_random_sg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auc = {'Dataset':  ['settle', 'stay'],\n",
    "        'AUC': [roc_auc_score_sns, roc_auc_score_sg],\n",
    "        'precision' : [precision_score_sns, precision_score_sg],\n",
    "        'accuracy' : [accuracy_score_sns, accuracy_score_sg],\n",
    "         'recall' : [recall_score_sns, recall_score_sg],\n",
    "            'f1': [f1_score_sns, f1_score_sg]\n",
    "        }\n",
    "data_auc = pd.DataFrame(data_auc, columns = ['Dataset', 'AUC', 'recall', 'precision', 'accuracy', 'f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_sns['Sort'] = pd.Series('Settle', index=feature_importance_sns.index)\n",
    "feature_importance_sg['Sort'] = pd.Series('Stay', index=feature_importance_sg.index)\n",
    "\n",
    "feature_importance_sns = feature_importance_sns.sort_values('Importance', ascending = False)\n",
    "feature_importance_sg = feature_importance_sg.sort_values('Importance', ascending = False)\n",
    "\n",
    "feature_importance = feature_importance_sns.append(feature_importance_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auc.to_csv ('../OUTPUT/rf_metrics_scores', index = False, header=True)\n",
    "feature_importance.to_csv ('../OUTPUT/rf_feature_importance', index = False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
